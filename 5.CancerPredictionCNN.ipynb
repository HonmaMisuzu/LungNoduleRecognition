{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.肺结节癌性预测\n",
    "\n",
    "* 在每个病人样本中找到最大的结节并在其上进行64x64的裁剪\n",
    "* 通过标签数据标记为癌性或非癌性，并创建相同数量的随机标签\n",
    "* 使用分层K折（K-fold）划分数据以进行交叉验证\n",
    "* 使用CNN进行癌性预测，交叉验证与消融实验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EDIT HERE##############################\n",
    "\n",
    "# 下面是从DetectNodules.ipynb获取到的结节图像、掩码数据及其特征表的路径\n",
    "\n",
    "DATAFOLDER = 'F:\\\\Datasets\\\\DSB3-processed\\\\'\n",
    "imageslist=[f'{DATAFOLDER}DSBNoduleImages.npy']\n",
    "maskslist=[f'{DATAFOLDER}DSBNoduleMasks.npy']\n",
    "tablelist=[f'{DATAFOLDER}DSBNoduleFeatures.csv']\n",
    "\n",
    "########################################\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "# 尝试使用可扩展的内存段来避免CUDA内存碎片化\n",
    "# 请注意，这可能会导致性能下降，因为这会导致更多的内存分配和释放操作\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "# 使用cuda\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.set_device(0)\n",
    "torch.backends.cudnn.benchmark = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 获取图像中的肺结节切片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 获取每个图像的最大结节，对处理过的图像进行64x64的裁剪，并用良恶性标签进行标记。\n",
    "from ProcessCTData import processimagenomask, crop_nodule, largestnodulearea, largestnodulecoordinates\n",
    "\n",
    "table = pd.read_csv(tablelist[0])\n",
    "if len(tablelist) > 1:\n",
    "    for file in tablelist[1:]:\n",
    "        temptable = pd.read_csv(file)\n",
    "        table = pd.concat([table, temptable])\n",
    "table = table.reset_index()\n",
    "\n",
    "print(\"Top 10 of DSBNoduleFeatures.csv :\\n\", table[:10])\n",
    "\n",
    "malignantlabel = []\n",
    "malignancytable = pd.concat([pd.read_csv(\"DSB/stage1_labels.csv\"), \n",
    "                             pd.read_csv(\"DSB/stage1_solution.csv\")])\n",
    "patients = malignancytable[\"id\"].values\n",
    "index = 0\n",
    "noduleexists = []\n",
    "nodulecrops = np.ndarray([len(patients), 1, 64, 64])\n",
    "indicies = []\n",
    "\n",
    "for i in range(len(imageslist)):\n",
    "    print(\"loading file\", imageslist[i])\n",
    "    noduleimages = np.load(imageslist[i])\n",
    "    nodulemasks = np.load(maskslist[i])\n",
    "    tabletemp = pd.read_csv(tablelist[i])\n",
    "    biggestnodulearea = []\n",
    "\n",
    "    for j in range(nodulemasks.shape[0]):\n",
    "        biggestnodulearea.append(largestnodulearea(nodulemasks[j, 0], tabletemp, j))\n",
    "\n",
    "    tabletemp[\"LargestNoduleArea\"] = pd.Series(biggestnodulearea)\n",
    "\n",
    "    for patient in tqdm(patients):\n",
    "        print(\"Process patient \", patient)\n",
    "        nodulearea = tabletemp[[\"LargestNoduleArea\"]].loc[tabletemp[\"Patient\"] == patient]\n",
    "\n",
    "        if len(nodulearea) > 0:\n",
    "            malignantlabel.append(malignancytable[\"cancer\"].loc[malignancytable[\"id\"] == patient].values[0].astype(bool))\n",
    "            noduleexists.append(1)\n",
    "            indx = nodulearea.loc[nodulearea[\"LargestNoduleArea\"] == max(nodulearea[\"LargestNoduleArea\"])].index[0]\n",
    "            indicies.append(indx)\n",
    "            nodcrop = crop_nodule(largestnodulecoordinates(nodulemasks[indx, 0]), processimagenomask(noduleimages[indx, 0]))\n",
    "\n",
    "            if nodcrop.shape[0] * nodcrop.shape[1] < 64 ** 2:\n",
    "                nodulecrops[index, 0] = np.zeros([64, 64])\n",
    "                nodulecrops[index, 0][0:nodcrop.shape[0], 0:nodcrop.shape[1]] = nodcrop\n",
    "            else:\n",
    "                nodulecrops[index, 0] = nodcrop\n",
    "\n",
    "            index += 1\n",
    "\n",
    "nodulecrops = nodulecrops[:index]\n",
    "# nodulecrops = nodulecrops.reshape(nodulecrops.shape[0], 64, 64, 1)\n",
    "# features = table.iloc[indicies]\n",
    "# features[\"label\"] = malignantlabel\n",
    "TFratio = len([a for a in malignantlabel if a == True]) / len(malignantlabel)\n",
    "print(\"Percent labels True: \", TFratio)\n",
    "\n",
    "randomlabel = np.random.choice([0, 1], size=(len(malignantlabel),), p=[(1 - TFratio), TFratio])\n",
    "malignantlabel = np.array(malignantlabel, dtype=bool)\n",
    "randomlabel = np.array(randomlabel, dtype=bool)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 显示结节图像\n",
    "\n",
    "fig, ax = plt.subplots(1, 4, figsize=(20, 4))\n",
    "\n",
    "for i, j in enumerate((10, 22, 46, 60)):\n",
    "    ax[i].imshow(nodulecrops[j, 0])\n",
    "    ax[i].axis(\"off\")\n",
    "    ax[i].set_title(\"Malignant: \" + str(malignantlabel[j]))\n",
    "# for i in range(5):\n",
    "#     ax[i].imshow(nodulecrops[i, 0])\n",
    "#     ax[i].axis(\"off\")\n",
    "#     ax[i].set_title(\"Malignant: \" + str(malignantlabel[i]))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义基础CNN和各参数，进行训练与验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义CNN模型\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, num_classes, width):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.width = width\n",
    "        self.conv1 = nn.Conv2d(1, width, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(width, width*2, kernel_size=3)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout1 = nn.Dropout(0.50)\n",
    "        self.fc1 = nn.Linear(width*2*30*30, width*4)\n",
    "        self.dropout2 = nn.Dropout(0.50)\n",
    "        self.fc2 = nn.Linear(width*4, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))             # 64->62\n",
    "        x = self.pool1(torch.relu(self.conv2(x))) # 62->60->30\n",
    "        x = self.dropout1(x)\n",
    "        x = x.view(-1, self.width*2*30*30)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "# 定义数据集\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 设置参数\n",
    "modelpath = 'cancerpredpths/'\n",
    "n_splits = 5\n",
    "kfold = StratifiedKFold(n_splits=n_splits, shuffle=True)\n",
    "n_kfold = 1\n",
    "num_classes = 2\n",
    "width = 32\n",
    "epochs = 200\n",
    "batch_size = 512\n",
    "cvscores = []\n",
    "cvscoresrandom = []\n",
    "history = []\n",
    "historyrandom = []\n",
    "aucscores = []\n",
    "aucscoresrandom = []\n",
    "predicted = []\n",
    "malignantlabeltest = []\n",
    "predictedrandom = []\n",
    "randomlabeltest = []\n",
    "\n",
    "# Convert the data to PyTorch tensors\n",
    "nodulecrops = torch.from_numpy(nodulecrops).float()\n",
    "malignantlabel = F.one_hot(torch.from_numpy(malignantlabel).long(), 2).float()\n",
    "randomlabel = F.one_hot(torch.from_numpy(randomlabel).long(), 2).float()\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "for train, test in tqdm(kfold.split(nodulecrops, malignantlabel[:,1]), total=n_splits):\n",
    "    os.makedirs(modelpath + f'k-fold-normal-{n_kfold}/', exist_ok=True)\n",
    "    print(f'Fold {n_kfold}/{n_splits}')\n",
    "    # Create the model\n",
    "    model = CNNModel(num_classes, width).cuda()\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "    # Create data loaders for training and validation\n",
    "    train_dataset = CustomDataset(nodulecrops[train], malignantlabel[train])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataset = CustomDataset(nodulecrops[test], malignantlabel[test])\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Train the model\n",
    "    train_loss_history = []\n",
    "    train_acc_history = []\n",
    "    val_loss_history = []\n",
    "    val_acc_history = []\n",
    "\n",
    "    # Initialize the best validation loss\n",
    "    best_train_loss = np.inf\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        total = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.requires_grad_(True)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            _, predicted_labels = torch.max(outputs, 1)\n",
    "            train_correct += (predicted_labels == labels[:,1]).sum().item()\n",
    "            total += predicted_labels.size(0)\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        train_accuracy = train_correct / total\n",
    "        train_loss_history.append(train_loss)\n",
    "        train_acc_history.append(train_accuracy)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs = inputs.cuda()\n",
    "                labels = labels.cuda()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted_labels = torch.max(outputs, 1)\n",
    "                val_correct += (predicted_labels == labels[:,1]).sum().item()\n",
    "                total += predicted_labels.size(0)\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_accuracy = val_correct / total\n",
    "        val_loss_history.append(val_loss)\n",
    "        val_acc_history.append(val_accuracy)\n",
    "\n",
    "        # print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}')\n",
    "        if train_loss < best_train_loss:\n",
    "            print(f'Epoch {epoch + 1}/{epochs}')\n",
    "            print(f'Train_loss decrease from {best_train_loss:.4f} to {train_loss:.4f}. Saving model...')\n",
    "            best_train_loss = train_loss\n",
    "            torch.save(model.state_dict(), modelpath + f'k-fold-normal-{n_kfold}/' + 'model_best.pth')\n",
    "    history.append({\n",
    "        'train_loss': train_loss_history,\n",
    "        'train_acc': train_acc_history,\n",
    "        'val_loss': val_loss_history,\n",
    "        'val_acc': val_acc_history\n",
    "    })\n",
    "\n",
    "    # Evaluate the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(nodulecrops[test].cuda())\n",
    "        _, predicted_labels = torch.max(outputs, 1)\n",
    "        val_loss = criterion(outputs, malignantlabel[test].cuda()).item()\n",
    "        val_accuracy = (predicted_labels.cpu() == malignantlabel[test][:,1]).sum().item() / len(malignantlabel[test][:,1])\n",
    "        cvscores.append([val_loss, val_accuracy])\n",
    "        predicted.append(outputs[:, 1].cpu().numpy())\n",
    "        malignantlabeltest.append(malignantlabel[test][:,1])\n",
    "        aucscores.append(roc_auc_score(malignantlabel[test][:,1].cpu().numpy(), outputs[:, 1].cpu().numpy()))\n",
    "\n",
    "    # Save the final model\n",
    "    torch.save(model.state_dict(), modelpath + f'k-fold-normal-{n_kfold}/' + 'model_final.pth')\n",
    "    n_kfold += 1\n",
    "\n",
    "predicted = np.concatenate(predicted, axis=0)\n",
    "malignantlabeltest = np.concatenate(malignantlabeltest, axis=0)\n",
    "roc = roc_curve(malignantlabeltest, predicted)\n",
    "\n",
    "n_kfold = 1\n",
    "\n",
    "# Perform k-fold cross-validation for random labels\n",
    "for train, test in tqdm(kfold.split(nodulecrops, randomlabel[:,1]), total=n_splits):\n",
    "    os.makedirs(modelpath + f'k-fold-random-{n_kfold}/', exist_ok=True)\n",
    "    print(f'Fold {n_kfold}/{n_splits}')\n",
    "    # Create the model\n",
    "    model = CNNModel(num_classes, width).cuda()\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "    # Create data loaders for training and validation\n",
    "    train_dataset = CustomDataset(nodulecrops[train], randomlabel[train])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataset = CustomDataset(nodulecrops[test], randomlabel[test])\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Train the model\n",
    "    train_loss_history = []\n",
    "    train_acc_history = []\n",
    "    val_loss_history = []\n",
    "    val_acc_history = []\n",
    "\n",
    "    # Initialize the best validation loss\n",
    "    best_train_loss = np.inf\n",
    "    \n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        total = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.requires_grad_(True)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            _, predicted_labels = torch.max(outputs, 1)\n",
    "            train_correct += (predicted_labels == labels[:,1]).sum().item()\n",
    "            total += predicted_labels.size(0)\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        train_accuracy = train_correct / total\n",
    "        train_loss_history.append(train_loss)\n",
    "        train_acc_history.append(train_accuracy)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs = inputs.cuda()\n",
    "                labels = labels.cuda()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted_labels = torch.max(outputs, 1)\n",
    "                val_correct += (predicted_labels == labels[:,1]).sum().item()\n",
    "                total += predicted_labels.size(0)\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_accuracy = val_correct / total\n",
    "        val_loss_history.append(val_loss)\n",
    "        val_acc_history.append(val_accuracy)\n",
    "\n",
    "        # print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}')\n",
    "        if train_loss < best_train_loss:\n",
    "            print(f'Epoch {epoch + 1}/{epochs}')\n",
    "            print(f'Train_loss decrease from {best_train_loss:.4f} to {train_loss:.4f}. Saving model...')\n",
    "            best_train_loss = train_loss\n",
    "            torch.save(model.state_dict(), modelpath + f'k-fold-random-{n_kfold}/' + 'model_best.pth')\n",
    "\n",
    "    historyrandom.append({\n",
    "        'train_loss': train_loss_history,\n",
    "        'train_acc': train_acc_history,\n",
    "        'val_loss': val_loss_history,\n",
    "        'val_acc': val_acc_history\n",
    "    })\n",
    "\n",
    "    # Evaluate the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(nodulecrops[test].cuda())\n",
    "        _, predicted_labels = torch.max(outputs, 1)\n",
    "        val_loss = criterion(outputs, randomlabel[test].cuda()).item()\n",
    "        val_accuracy = (predicted_labels.cpu() == randomlabel[test][:,1]).sum().item() / len(randomlabel[test][:,1])\n",
    "        cvscoresrandom.append([val_loss, val_accuracy])\n",
    "        predictedrandom.append(outputs[:, 1].cpu().numpy())\n",
    "        randomlabeltest.append(randomlabel[test][:,1])\n",
    "        aucscoresrandom.append(roc_auc_score(randomlabel[test][:,1].cpu().numpy(), outputs[:, 1].cpu().numpy()))\n",
    "    \n",
    "    # Save the final model\n",
    "    torch.save(model.state_dict(), modelpath + f'k-fold-random-{n_kfold}/' + 'model_final.pth')\n",
    "    n_kfold += 1\n",
    "\n",
    "predictedrandom = np.concatenate(predictedrandom, axis=0)\n",
    "randomlabeltest = np.concatenate(randomlabeltest, axis=0)\n",
    "rocrandom = roc_curve(randomlabeltest, predictedrandom)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通过各种方法评估模型\n",
    "\n",
    "print(\"Mean loss across all CV sets with true labels:\", np.mean([cvscores[i][0] for i in range(len(cvscores))]))\n",
    "print(\"Mean loss across all CV sets with random labels:\", np.mean([cvscoresrandom[i][0] for i in range(len(cvscoresrandom))]))\n",
    "print(\"Mean accuracy across all CV sets with true labels:\", np.mean([cvscores[i][1] for i in range(len(cvscores))]))\n",
    "print(\"Mean accuracy across all CV sets with random labels:\", np.mean([cvscoresrandom[i][1] for i in range(len(cvscoresrandom))]))\n",
    "\n",
    "print(\"Lowest val_loss of\", min(np.mean([history[i]['val_loss'] for i in range(n_splits)],axis=0)), \"at epoch\", np.where(np.mean([history[i]['val_loss'] for i in range(n_splits)],axis=0)==min(np.mean([history[i]['val_loss'] for i in range(n_splits)],axis=0)))[0], \"with true labels\")\n",
    "print(\"Lowest val_loss of\", min(np.mean([historyrandom[i]['val_loss'] for i in range(n_splits)],axis=0)), \"at epoch\", np.where(np.mean([historyrandom[i]['val_loss'] for i in range(n_splits)],axis=0)==min(np.mean([historyrandom[i]['val_loss'] for i in range(n_splits)],axis=0)))[0],\"with random labels\")\n",
    "print(\"Average AUC across CV sets with true labels:\",np.mean(aucscores))\n",
    "print(\"Average AUC across CV sets with random labels:\",np.mean(aucscoresrandom))\n",
    "acc=np.mean([history[i]['train_acc'] for i in range(n_splits)], axis=0)\n",
    "valacc=np.mean([history[i]['val_acc'] for i in range(n_splits)], axis=0)\n",
    "loss=np.mean([history[i]['train_loss'] for i in range(n_splits)], axis=0)\n",
    "valloss=np.mean([history[i]['val_loss'] for i in range(n_splits)], axis=0)\n",
    "randacc=np.mean([historyrandom[i]['train_acc'] for i in range(n_splits)], axis=0)\n",
    "randvalacc=np.mean([historyrandom[i]['val_acc'] for i in range(n_splits)], axis=0)\n",
    "randloss=np.mean([historyrandom[i]['train_loss'] for i in range(n_splits)], axis=0)\n",
    "randvalloss=np.mean([historyrandom[i]['val_loss'] for i in range(n_splits)], axis=0)\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(loss)\n",
    "plt.plot(valloss)\n",
    "plt.plot(randloss)\n",
    "plt.plot(randvalloss)\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train truelabel', 'validation truelabel', 'train randomlabel', 'val randomlabel'], loc='best')\n",
    "plt.ylim([0.45,0.7])\n",
    "plt.show()\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(acc)\n",
    "plt.plot(valacc)\n",
    "plt.plot(randacc)\n",
    "plt.plot(randvalacc)\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train truelabel', 'validation truelabel', 'train randomlabel', 'val randomlabel'], loc='best')\n",
    "plt.ylim([0.7,0.78])\n",
    "plt.show()\n",
    "\n",
    "#ROC curve\n",
    "plt.plot(roc[0],roc[1])\n",
    "plt.plot(rocrandom[0],rocrandom[1])\n",
    "plt.title('ROC')\n",
    "plt.ylabel('TPrate')\n",
    "plt.xlabel('FPrate')\n",
    "plt.legend(['true label', 'random label'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 添加一层卷积层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义CNN模型\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, num_classes, width):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.width = width\n",
    "        self.conv1 = nn.Conv2d(1, width, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(width, width*2, kernel_size=3)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv3 = nn.Conv2d(width*2, width*2, kernel_size=3, padding=1) # 添加一层卷积层\n",
    "        self.dropout1 = nn.Dropout(0.50)\n",
    "        self.fc1 = nn.Linear(width*2*30*30, width*4)\n",
    "        self.dropout2 = nn.Dropout(0.50)\n",
    "        self.fc2 = nn.Linear(width*4, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))             # 64->62\n",
    "        x = self.pool1(torch.relu(self.conv2(x))) # 62->60->30\n",
    "        x = torch.relu(self.conv3(x))             # 30->30\n",
    "        x = self.dropout1(x)\n",
    "        x = x.view(-1, self.width*2*30*30)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "# 定义数据集\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 设置参数\n",
    "modelpath = 'cancerpredpths1/'\n",
    "n_splits = 5\n",
    "kfold = StratifiedKFold(n_splits=n_splits, shuffle=True)\n",
    "n_kfold = 1\n",
    "num_classes = 2\n",
    "width = 32\n",
    "epochs = 200\n",
    "batch_size = 512\n",
    "cvscores = []\n",
    "cvscoresrandom = []\n",
    "history = []\n",
    "historyrandom = []\n",
    "aucscores = []\n",
    "aucscoresrandom = []\n",
    "predicted = []\n",
    "malignantlabeltest = []\n",
    "predictedrandom = []\n",
    "randomlabeltest = []\n",
    "\n",
    "# Convert the data to PyTorch tensors\n",
    "# nodulecrops = torch.from_numpy(nodulecrops).float()\n",
    "# malignantlabel = F.one_hot(torch.from_numpy(malignantlabel).long(), 2).float()\n",
    "# randomlabel = F.one_hot(torch.from_numpy(randomlabel).long(), 2).float()\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "for train, test in tqdm(kfold.split(nodulecrops, malignantlabel[:,1]), total=n_splits):\n",
    "    os.makedirs(modelpath + f'k-fold-normal-{n_kfold}/', exist_ok=True)\n",
    "    print(f'Fold {n_kfold}/{n_splits}')\n",
    "    # Create the model\n",
    "    model = CNNModel(num_classes, width).cuda()\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "    # Create data loaders for training and validation\n",
    "    train_dataset = CustomDataset(nodulecrops[train], malignantlabel[train])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataset = CustomDataset(nodulecrops[test], malignantlabel[test])\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Train the model\n",
    "    train_loss_history = []\n",
    "    train_acc_history = []\n",
    "    val_loss_history = []\n",
    "    val_acc_history = []\n",
    "\n",
    "    # Initialize the best validation loss\n",
    "    best_train_loss = np.inf\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        total = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.requires_grad_(True)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            _, predicted_labels = torch.max(outputs, 1)\n",
    "            train_correct += (predicted_labels == labels[:,1]).sum().item()\n",
    "            total += predicted_labels.size(0)\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        train_accuracy = train_correct / total\n",
    "        train_loss_history.append(train_loss)\n",
    "        train_acc_history.append(train_accuracy)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs = inputs.cuda()\n",
    "                labels = labels.cuda()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted_labels = torch.max(outputs, 1)\n",
    "                val_correct += (predicted_labels == labels[:,1]).sum().item()\n",
    "                total += predicted_labels.size(0)\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_accuracy = val_correct / total\n",
    "        val_loss_history.append(val_loss)\n",
    "        val_acc_history.append(val_accuracy)\n",
    "\n",
    "        # print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}')\n",
    "        if train_loss < best_train_loss:\n",
    "            print(f'Epoch {epoch + 1}/{epochs}')\n",
    "            print(f'Train_loss decrease from {best_train_loss:.4f} to {train_loss:.4f}. Saving model...')\n",
    "            best_train_loss = train_loss\n",
    "            torch.save(model.state_dict(), modelpath + f'k-fold-normal-{n_kfold}/' + 'model_best.pth')\n",
    "    history.append({\n",
    "        'train_loss': train_loss_history,\n",
    "        'train_acc': train_acc_history,\n",
    "        'val_loss': val_loss_history,\n",
    "        'val_acc': val_acc_history\n",
    "    })\n",
    "\n",
    "    # Evaluate the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(nodulecrops[test].cuda())\n",
    "        _, predicted_labels = torch.max(outputs, 1)\n",
    "        val_loss = criterion(outputs, malignantlabel[test].cuda()).item()\n",
    "        val_accuracy = (predicted_labels.cpu() == malignantlabel[test][:,1]).sum().item() / len(malignantlabel[test][:,1])\n",
    "        cvscores.append([val_loss, val_accuracy])\n",
    "        predicted.append(outputs[:, 1].cpu().numpy())\n",
    "        malignantlabeltest.append(malignantlabel[test][:,1])\n",
    "        aucscores.append(roc_auc_score(malignantlabel[test][:,1].cpu().numpy(), outputs[:, 1].cpu().numpy()))\n",
    "\n",
    "    # Save the final model\n",
    "    torch.save(model.state_dict(), modelpath + f'k-fold-normal-{n_kfold}/' + 'model_final.pth')\n",
    "    n_kfold += 1\n",
    "\n",
    "predicted = np.concatenate(predicted, axis=0)\n",
    "malignantlabeltest = np.concatenate(malignantlabeltest, axis=0)\n",
    "roc = roc_curve(malignantlabeltest, predicted)\n",
    "\n",
    "n_kfold = 1\n",
    "\n",
    "# Perform k-fold cross-validation for random labels\n",
    "for train, test in tqdm(kfold.split(nodulecrops, randomlabel[:,1]), total=n_splits):\n",
    "    os.makedirs(modelpath + f'k-fold-random-{n_kfold}/', exist_ok=True)\n",
    "    print(f'Fold {n_kfold}/{n_splits}')\n",
    "    # Create the model\n",
    "    model = CNNModel(num_classes, width).cuda()\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "    # Create data loaders for training and validation\n",
    "    train_dataset = CustomDataset(nodulecrops[train], randomlabel[train])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataset = CustomDataset(nodulecrops[test], randomlabel[test])\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Train the model\n",
    "    train_loss_history = []\n",
    "    train_acc_history = []\n",
    "    val_loss_history = []\n",
    "    val_acc_history = []\n",
    "\n",
    "    # Initialize the best validation loss\n",
    "    best_train_loss = np.inf\n",
    "    \n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        total = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.requires_grad_(True)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            _, predicted_labels = torch.max(outputs, 1)\n",
    "            train_correct += (predicted_labels == labels[:,1]).sum().item()\n",
    "            total += predicted_labels.size(0)\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        train_accuracy = train_correct / total\n",
    "        train_loss_history.append(train_loss)\n",
    "        train_acc_history.append(train_accuracy)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs = inputs.cuda()\n",
    "                labels = labels.cuda()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted_labels = torch.max(outputs, 1)\n",
    "                val_correct += (predicted_labels == labels[:,1]).sum().item()\n",
    "                total += predicted_labels.size(0)\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_accuracy = val_correct / total\n",
    "        val_loss_history.append(val_loss)\n",
    "        val_acc_history.append(val_accuracy)\n",
    "\n",
    "        # print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}')\n",
    "        if train_loss < best_train_loss:\n",
    "            print(f'Epoch {epoch + 1}/{epochs}')\n",
    "            print(f'Train_loss decrease from {best_train_loss:.4f} to {train_loss:.4f}. Saving model...')\n",
    "            best_train_loss = train_loss\n",
    "            torch.save(model.state_dict(), modelpath + f'k-fold-random-{n_kfold}/' + 'model_best.pth')\n",
    "\n",
    "    historyrandom.append({\n",
    "        'train_loss': train_loss_history,\n",
    "        'train_acc': train_acc_history,\n",
    "        'val_loss': val_loss_history,\n",
    "        'val_acc': val_acc_history\n",
    "    })\n",
    "\n",
    "    # Evaluate the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(nodulecrops[test].cuda())\n",
    "        _, predicted_labels = torch.max(outputs, 1)\n",
    "        val_loss = criterion(outputs, randomlabel[test].cuda()).item()\n",
    "        val_accuracy = (predicted_labels.cpu() == randomlabel[test][:,1]).sum().item() / len(randomlabel[test][:,1])\n",
    "        cvscoresrandom.append([val_loss, val_accuracy])\n",
    "        predictedrandom.append(outputs[:, 1].cpu().numpy())\n",
    "        randomlabeltest.append(randomlabel[test][:,1])\n",
    "        aucscoresrandom.append(roc_auc_score(randomlabel[test][:,1].cpu().numpy(), outputs[:, 1].cpu().numpy()))\n",
    "    \n",
    "    # Save the final model\n",
    "    torch.save(model.state_dict(), modelpath + f'k-fold-random-{n_kfold}/' + 'model_final.pth')\n",
    "    n_kfold += 1\n",
    "\n",
    "predictedrandom = np.concatenate(predictedrandom, axis=0)\n",
    "randomlabeltest = np.concatenate(randomlabeltest, axis=0)\n",
    "rocrandom = roc_curve(randomlabeltest, predictedrandom)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean loss across all CV sets with true labels:\", np.mean([cvscores[i][0] for i in range(len(cvscores))]))\n",
    "print(\"Mean loss across all CV sets with random labels:\", np.mean([cvscoresrandom[i][0] for i in range(len(cvscoresrandom))]))\n",
    "print(\"Mean accuracy across all CV sets with true labels:\", np.mean([cvscores[i][1] for i in range(len(cvscores))]))\n",
    "print(\"Mean accuracy across all CV sets with random labels:\", np.mean([cvscoresrandom[i][1] for i in range(len(cvscoresrandom))]))\n",
    "\n",
    "print(\"Lowest val_loss of\", min(np.mean([history[i]['val_loss'] for i in range(n_splits)],axis=0)), \"at epoch\", np.where(np.mean([history[i]['val_loss'] for i in range(n_splits)],axis=0)==min(np.mean([history[i]['val_loss'] for i in range(n_splits)],axis=0)))[0], \"with true labels\")\n",
    "print(\"Lowest val_loss of\", min(np.mean([historyrandom[i]['val_loss'] for i in range(n_splits)],axis=0)), \"at epoch\", np.where(np.mean([historyrandom[i]['val_loss'] for i in range(n_splits)],axis=0)==min(np.mean([historyrandom[i]['val_loss'] for i in range(n_splits)],axis=0)))[0],\"with random labels\")\n",
    "print(\"Average AUC across CV sets with true labels:\",np.mean(aucscores))\n",
    "print(\"Average AUC across CV sets with random labels:\",np.mean(aucscoresrandom))\n",
    "acc=np.mean([history[i]['train_acc'] for i in range(n_splits)], axis=0)\n",
    "valacc=np.mean([history[i]['val_acc'] for i in range(n_splits)], axis=0)\n",
    "loss=np.mean([history[i]['train_loss'] for i in range(n_splits)], axis=0)\n",
    "valloss=np.mean([history[i]['val_loss'] for i in range(n_splits)], axis=0)\n",
    "randacc=np.mean([historyrandom[i]['train_acc'] for i in range(n_splits)], axis=0)\n",
    "randvalacc=np.mean([historyrandom[i]['val_acc'] for i in range(n_splits)], axis=0)\n",
    "randloss=np.mean([historyrandom[i]['train_loss'] for i in range(n_splits)], axis=0)\n",
    "randvalloss=np.mean([historyrandom[i]['val_loss'] for i in range(n_splits)], axis=0)\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(loss)\n",
    "plt.plot(valloss)\n",
    "plt.plot(randloss)\n",
    "plt.plot(randvalloss)\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train truelabel', 'validation truelabel', 'train randomlabel', 'val randomlabel'], loc='best')\n",
    "plt.ylim([0.45,0.7])\n",
    "plt.show()\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(acc)\n",
    "plt.plot(valacc)\n",
    "plt.plot(randacc)\n",
    "plt.plot(randvalacc)\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train truelabel', 'validation truelabel', 'train randomlabel', 'val randomlabel'], loc='best')\n",
    "plt.ylim([0.7,0.78])\n",
    "plt.show()\n",
    "\n",
    "#ROC curve\n",
    "plt.plot(roc[0],roc[1])\n",
    "plt.plot(rocrandom[0],rocrandom[1])\n",
    "plt.title('ROC')\n",
    "plt.ylabel('TPrate')\n",
    "plt.xlabel('FPrate')\n",
    "plt.legend(['true label', 'random label'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 再将第一个dropout层修改为0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义CNN模型\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, num_classes, width):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.width = width\n",
    "        self.conv1 = nn.Conv2d(1, width, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(width, width*2, kernel_size=3)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv3 = nn.Conv2d(width*2, width*2, kernel_size=3, padding=1) # 添加一层卷积层\n",
    "        self.dropout1 = nn.Dropout(0.75)                       # 修改dropout 0.75\n",
    "        self.fc1 = nn.Linear(width*2*30*30, width*4)\n",
    "        self.dropout2 = nn.Dropout(0.50)\n",
    "        self.fc2 = nn.Linear(width*4, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))             # 64->62\n",
    "        x = self.pool1(torch.relu(self.conv2(x))) # 62->60->30\n",
    "        x = torch.relu(self.conv3(x))             # 30->30\n",
    "        x = self.dropout1(x)\n",
    "        x = x.view(-1, self.width*2*30*30)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "# 定义数据集\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置参数\n",
    "modelpath = 'cancerpredpths2/'\n",
    "n_splits = 5\n",
    "kfold = StratifiedKFold(n_splits=n_splits, shuffle=True)\n",
    "n_kfold = 1\n",
    "num_classes = 2\n",
    "width = 32\n",
    "epochs = 200\n",
    "batch_size = 512\n",
    "cvscores = []\n",
    "cvscoresrandom = []\n",
    "history = []\n",
    "historyrandom = []\n",
    "aucscores = []\n",
    "aucscoresrandom = []\n",
    "predicted = []\n",
    "malignantlabeltest = []\n",
    "predictedrandom = []\n",
    "randomlabeltest = []\n",
    "\n",
    "# Convert the data to PyTorch tensors\n",
    "# nodulecrops = torch.from_numpy(nodulecrops).float()\n",
    "# malignantlabel = F.one_hot(torch.from_numpy(malignantlabel).long(), 2).float()\n",
    "# randomlabel = F.one_hot(torch.from_numpy(randomlabel).long(), 2).float()\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "for train, test in tqdm(kfold.split(nodulecrops, malignantlabel[:,1]), total=n_splits):\n",
    "    os.makedirs(modelpath + f'k-fold-normal-{n_kfold}/', exist_ok=True)\n",
    "    print(f'Fold {n_kfold}/{n_splits}')\n",
    "    # Create the model\n",
    "    model = CNNModel(num_classes, width).cuda()\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "    # Create data loaders for training and validation\n",
    "    train_dataset = CustomDataset(nodulecrops[train], malignantlabel[train])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataset = CustomDataset(nodulecrops[test], malignantlabel[test])\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Train the model\n",
    "    train_loss_history = []\n",
    "    train_acc_history = []\n",
    "    val_loss_history = []\n",
    "    val_acc_history = []\n",
    "\n",
    "    # Initialize the best validation loss\n",
    "    best_train_loss = np.inf\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        total = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.requires_grad_(True)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            _, predicted_labels = torch.max(outputs, 1)\n",
    "            train_correct += (predicted_labels == labels[:,1]).sum().item()\n",
    "            total += predicted_labels.size(0)\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        train_accuracy = train_correct / total\n",
    "        train_loss_history.append(train_loss)\n",
    "        train_acc_history.append(train_accuracy)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs = inputs.cuda()\n",
    "                labels = labels.cuda()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted_labels = torch.max(outputs, 1)\n",
    "                val_correct += (predicted_labels == labels[:,1]).sum().item()\n",
    "                total += predicted_labels.size(0)\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_accuracy = val_correct / total\n",
    "        val_loss_history.append(val_loss)\n",
    "        val_acc_history.append(val_accuracy)\n",
    "\n",
    "        # print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}')\n",
    "        if train_loss < best_train_loss:\n",
    "            print(f'Epoch {epoch + 1}/{epochs}')\n",
    "            print(f'Train_loss decrease from {best_train_loss:.4f} to {train_loss:.4f}. Saving model...')\n",
    "            best_train_loss = train_loss\n",
    "            torch.save(model.state_dict(), modelpath + f'k-fold-normal-{n_kfold}/' + 'model_best.pth')\n",
    "    history.append({\n",
    "        'train_loss': train_loss_history,\n",
    "        'train_acc': train_acc_history,\n",
    "        'val_loss': val_loss_history,\n",
    "        'val_acc': val_acc_history\n",
    "    })\n",
    "\n",
    "    # Evaluate the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(nodulecrops[test].cuda())\n",
    "        _, predicted_labels = torch.max(outputs, 1)\n",
    "        val_loss = criterion(outputs, malignantlabel[test].cuda()).item()\n",
    "        val_accuracy = (predicted_labels.cpu() == malignantlabel[test][:,1]).sum().item() / len(malignantlabel[test][:,1])\n",
    "        cvscores.append([val_loss, val_accuracy])\n",
    "        predicted.append(outputs[:, 1].cpu().numpy())\n",
    "        malignantlabeltest.append(malignantlabel[test][:,1])\n",
    "        aucscores.append(roc_auc_score(malignantlabel[test][:,1].cpu().numpy(), outputs[:, 1].cpu().numpy()))\n",
    "\n",
    "    # Save the final model\n",
    "    torch.save(model.state_dict(), modelpath + f'k-fold-normal-{n_kfold}/' + 'model_final.pth')\n",
    "    n_kfold += 1\n",
    "\n",
    "predicted = np.concatenate(predicted, axis=0)\n",
    "malignantlabeltest = np.concatenate(malignantlabeltest, axis=0)\n",
    "roc = roc_curve(malignantlabeltest, predicted)\n",
    "\n",
    "n_kfold = 1\n",
    "\n",
    "# Perform k-fold cross-validation for random labels\n",
    "for train, test in tqdm(kfold.split(nodulecrops, randomlabel[:,1]), total=n_splits):\n",
    "    os.makedirs(modelpath + f'k-fold-random-{n_kfold}/', exist_ok=True)\n",
    "    print(f'Fold {n_kfold}/{n_splits}')\n",
    "    # Create the model\n",
    "    model = CNNModel(num_classes, width).cuda()\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "    # Create data loaders for training and validation\n",
    "    train_dataset = CustomDataset(nodulecrops[train], randomlabel[train])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataset = CustomDataset(nodulecrops[test], randomlabel[test])\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Train the model\n",
    "    train_loss_history = []\n",
    "    train_acc_history = []\n",
    "    val_loss_history = []\n",
    "    val_acc_history = []\n",
    "\n",
    "    # Initialize the best validation loss\n",
    "    best_train_loss = np.inf\n",
    "    \n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        total = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.requires_grad_(True)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            _, predicted_labels = torch.max(outputs, 1)\n",
    "            train_correct += (predicted_labels == labels[:,1]).sum().item()\n",
    "            total += predicted_labels.size(0)\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        train_accuracy = train_correct / total\n",
    "        train_loss_history.append(train_loss)\n",
    "        train_acc_history.append(train_accuracy)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs = inputs.cuda()\n",
    "                labels = labels.cuda()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted_labels = torch.max(outputs, 1)\n",
    "                val_correct += (predicted_labels == labels[:,1]).sum().item()\n",
    "                total += predicted_labels.size(0)\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_accuracy = val_correct / total\n",
    "        val_loss_history.append(val_loss)\n",
    "        val_acc_history.append(val_accuracy)\n",
    "\n",
    "        # print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}')\n",
    "        if train_loss < best_train_loss:\n",
    "            print(f'Epoch {epoch + 1}/{epochs}')\n",
    "            print(f'Train_loss decrease from {best_train_loss:.4f} to {train_loss:.4f}. Saving model...')\n",
    "            best_train_loss = train_loss\n",
    "            torch.save(model.state_dict(), modelpath + f'k-fold-random-{n_kfold}/' + 'model_best.pth')\n",
    "\n",
    "    historyrandom.append({\n",
    "        'train_loss': train_loss_history,\n",
    "        'train_acc': train_acc_history,\n",
    "        'val_loss': val_loss_history,\n",
    "        'val_acc': val_acc_history\n",
    "    })\n",
    "\n",
    "    # Evaluate the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(nodulecrops[test].cuda())\n",
    "        _, predicted_labels = torch.max(outputs, 1)\n",
    "        val_loss = criterion(outputs, randomlabel[test].cuda()).item()\n",
    "        val_accuracy = (predicted_labels.cpu() == randomlabel[test][:,1]).sum().item() / len(randomlabel[test][:,1])\n",
    "        cvscoresrandom.append([val_loss, val_accuracy])\n",
    "        predictedrandom.append(outputs[:, 1].cpu().numpy())\n",
    "        randomlabeltest.append(randomlabel[test][:,1])\n",
    "        aucscoresrandom.append(roc_auc_score(randomlabel[test][:,1].cpu().numpy(), outputs[:, 1].cpu().numpy()))\n",
    "    \n",
    "    # Save the final model\n",
    "    torch.save(model.state_dict(), modelpath + f'k-fold-random-{n_kfold}/' + 'model_final.pth')\n",
    "    n_kfold += 1\n",
    "\n",
    "predictedrandom = np.concatenate(predictedrandom, axis=0)\n",
    "randomlabeltest = np.concatenate(randomlabeltest, axis=0)\n",
    "rocrandom = roc_curve(randomlabeltest, predictedrandom)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean loss across all CV sets with true labels:\", np.mean([cvscores[i][0] for i in range(len(cvscores))]))\n",
    "print(\"Mean loss across all CV sets with random labels:\", np.mean([cvscoresrandom[i][0] for i in range(len(cvscoresrandom))]))\n",
    "print(\"Mean accuracy across all CV sets with true labels:\", np.mean([cvscores[i][1] for i in range(len(cvscores))]))\n",
    "print(\"Mean accuracy across all CV sets with random labels:\", np.mean([cvscoresrandom[i][1] for i in range(len(cvscoresrandom))]))\n",
    "\n",
    "print(\"Lowest val_loss of\", min(np.mean([history[i]['val_loss'] for i in range(n_splits)],axis=0)), \"at epoch\", np.where(np.mean([history[i]['val_loss'] for i in range(n_splits)],axis=0)==min(np.mean([history[i]['val_loss'] for i in range(n_splits)],axis=0)))[0], \"with true labels\")\n",
    "print(\"Lowest val_loss of\", min(np.mean([historyrandom[i]['val_loss'] for i in range(n_splits)],axis=0)), \"at epoch\", np.where(np.mean([historyrandom[i]['val_loss'] for i in range(n_splits)],axis=0)==min(np.mean([historyrandom[i]['val_loss'] for i in range(n_splits)],axis=0)))[0],\"with random labels\")\n",
    "print(\"Average AUC across CV sets with true labels:\",np.mean(aucscores))\n",
    "print(\"Average AUC across CV sets with random labels:\",np.mean(aucscoresrandom))\n",
    "acc=np.mean([history[i]['train_acc'] for i in range(n_splits)], axis=0)\n",
    "valacc=np.mean([history[i]['val_acc'] for i in range(n_splits)], axis=0)\n",
    "loss=np.mean([history[i]['train_loss'] for i in range(n_splits)], axis=0)\n",
    "valloss=np.mean([history[i]['val_loss'] for i in range(n_splits)], axis=0)\n",
    "randacc=np.mean([historyrandom[i]['train_acc'] for i in range(n_splits)], axis=0)\n",
    "randvalacc=np.mean([historyrandom[i]['val_acc'] for i in range(n_splits)], axis=0)\n",
    "randloss=np.mean([historyrandom[i]['train_loss'] for i in range(n_splits)], axis=0)\n",
    "randvalloss=np.mean([historyrandom[i]['val_loss'] for i in range(n_splits)], axis=0)\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(loss)\n",
    "plt.plot(valloss)\n",
    "plt.plot(randloss)\n",
    "plt.plot(randvalloss)\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train truelabel', 'validation truelabel', 'train randomlabel', 'val randomlabel'], loc='best')\n",
    "plt.ylim([0.45,0.7])\n",
    "plt.show()\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(acc)\n",
    "plt.plot(valacc)\n",
    "plt.plot(randacc)\n",
    "plt.plot(randvalacc)\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train truelabel', 'validation truelabel', 'train randomlabel', 'val randomlabel'], loc='best')\n",
    "plt.ylim([0.7,0.78])\n",
    "plt.show()\n",
    "\n",
    "#ROC curve\n",
    "plt.plot(roc[0],roc[1])\n",
    "plt.plot(rocrandom[0],rocrandom[1])\n",
    "plt.title('ROC')\n",
    "plt.ylabel('TPrate')\n",
    "plt.xlabel('FPrate')\n",
    "plt.legend(['true label', 'random label'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
